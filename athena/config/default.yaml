# Default configuration for Athena Recursive AI MoE System
# Optimized GPU distribution for maximum throughput and true parallelism

# GPU Distribution Strategy:
# GPU 1 (0): Orchestrator + Reasoning + Memory (~18GB) - Logical/context-heavy workloads
# GPU 2 (1): Creative + Technical (~9GB) - Creative/implementation workloads
# This enables 1.5-2x tokens/s gains during multi-expert parallel queries

orchestrator:
  url: http://localhost:1234/v1
  model_name: qwen2.5-14b-instruct
  gpu_id: 0  # GPU 1: ~10GB
  max_tokens: 2048
  temperature: 0.7
  top_p: 0.9
  timeout: 120

reasoning_expert:
  url: http://localhost:1235/v1
  model_name: phi-3.5-mini-instruct
  gpu_id: 0  # GPU 1: ~3GB (logical reasoning with orchestrator)
  max_tokens: 2048
  temperature: 0.5  # Lower temperature for logical reasoning
  top_p: 0.9
  timeout: 60

memory_expert:
  url: http://localhost:1236/v1
  model_name: llama-3.1-8b-instruct
  gpu_id: 0  # GPU 1: ~5GB (context-heavy tasks)
  max_tokens: 2048
  temperature: 0.5
  top_p: 0.9
  timeout: 60

creative_expert:
  url: http://localhost:1237/v1
  model_name: mistral-7b-instruct
  gpu_id: 1  # GPU 2: ~4.5GB
  max_tokens: 2048
  temperature: 0.9  # Higher temperature for creativity
  top_p: 0.95
  timeout: 60

technical_expert:
  url: http://localhost:1238/v1
  model_name: codeqwen-7b-instruct
  gpu_id: 1  # GPU 2: ~4.5GB
  max_tokens: 4096  # More tokens for code generation
  temperature: 0.3  # Low temperature for precise code
  top_p: 0.9
  timeout: 90

gwt:
  attention_threshold: 0.6
  workspace_size: 5
  broadcast_decay: 0.9
  enable_competition: true

routing:
  enable_parallel: true
  confidence_threshold: 0.7
  max_retries: 3
  timeout_per_expert: 60

logging:
  level: INFO
  file: athena.log
  format: "<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan> - <level>{message}</level>"
  rotation: "10 MB"

max_context_length: 8192
conversation_history_size: 10
